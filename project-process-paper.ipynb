{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfAGHJHiatl5"
      },
      "source": [
        "# Enter Name here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEyyDPZVatl8"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "This process paper is a time for you to discuss the process you took personally to get to the final project. Why did you choose the data you chose? How well did it work for your question? Did you need (or want) to change your question as you evaluated your data more? What provided you with particular difficulties or struggles? How did you overcome those challenges? What parts were the easy parts?\n",
        "\n",
        "This paper should be a minimum of 300 words. It should be turned in via a .ipynb file, so you can demonstrate some of the issues with code, or you can use it to show things you thought were particularly interesting, but might not have found their way into the final project.\n",
        "\n",
        "Think of this as a reflection on the process you went through to get to the final project. You should be familiar with reflections at this point in your Queens career.\n",
        "\n",
        "Begin writing in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0Eyz1LGatl9"
      },
      "source": [
        "\n",
        "##Process Paper\n",
        "\n",
        "When I first started this project, I wanted to work with a dataset that felt relevant to my everyday life. The Netflix Movies and TV Shows dataset immediately caught my attention because Netflix is such a major part of entertainment now. I decided my main question would be whether Netflix has shifted toward adding more recently released content over the years — something I was genuinely curious about.\n",
        "\n",
        "The dataset seemed perfect at first glance, but as I worked with it, I realized there were a lot of technical issues that I had to solve before I could even get to the analysis. For example, one of the first problems was converting the 'date_added' column. Originally, I just tried:\n",
        "\n",
        "```python\n",
        "df['year_added'] = df['date_added'].year\n",
        "```\n",
        "\n",
        "which gave me an AttributeError because 'Series' object has no attribute 'year'.  \n",
        "After researching and looking through the dataset again, I realized I had to convert the column to a datetime format first, and use .dt.year properly:\n",
        "\n",
        "```python\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "```\n",
        "\n",
        "Another serious issue came when I tried to run the Pearson correlation. Initially, I wrote:\n",
        "\n",
        "```python\n",
        "r, p = pearsonr(df['release_year'], df['year_added'])\n",
        "```\n",
        "\n",
        "but it threw an error because both columns had NaN values. Just dropping them wasn’t enough because the NaNs were causing type inconsistencies in the memory. I had to specifically **drop NaNs first**, **force the columns into numeric format**, and **verify their datatypes**:\n",
        "\n",
        "```python\n",
        "pearson_df = df[['release_year', 'year_added']].dropna()\n",
        "pearson_df['release_year'] = pd.to_numeric(pearson_df['release_year'], errors='coerce')\n",
        "pearson_df['year_added'] = pd.to_numeric(pearson_df['year_added'], errors='coerce')\n",
        "```\n",
        "After that,  Pearson worked correctly and the scatterplot showed me a real trend.\n",
        "\n",
        "Another difficult part was dealing with the 'country' field for the international trends analysis. Countries were listed as comma-separated strings, so only a single show could be labeled under multiple countries. I had to split the string, split the rows into multiple entries, and filter only the top five countries dynamically using this:\n",
        "\n",
        "```python\n",
        "country_df = df.dropna(subset=['country', 'year_added'])\n",
        "country_df['country'] = country_df['country'].str.split(', ')\n",
        "country_expanded = country_df.explode('country')\n",
        "top_countries = country_expanded['country'].value_counts().head(5).index\n",
        "filtered = country_expanded[country_expanded['country'].isin(top_countries)]\n",
        "```\n",
        "\n",
        "This project definetly gave me a run for my money. I had to restructure the DataFrame entirely to get meaningful graphs out of it.\n",
        "\n",
        "Looking back, the hardest parts were definitely the hidden issues with data types, missing values, and reshaping data frames properly for analysis. The easier parts were creating the visualizations once the data was clean — Seaborn made it simple to create clean scatterplots, boxplots, and bar charts.\n",
        "\n",
        "I never ended up changing my research question because the dataset supported it so well. However, I did expand the scope by adding bonus analyses on content types and international expansion once I realized the dataset had more to offer.\n",
        "\n",
        "Overall, this project made me realize that getting the data ready is often harder than the actual analysis. Cleaning, validating, and reshaping the data took way more time than running correlations or t-tests. I feel like I learned a lot more about real-world data challenges, and not just “running code,” but actually understanding why something breaks and how to fix it properly.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}